# Posture Analyzer at the Edge â€” **Updated Scheduling** (KEDA + gRPC + Prometheus)

This repository turns a small Kubernetes cluster of Jetson/edge nodes into a **GPUâ€‘aware, selfâ€‘balancing posture analysis farm**.  
It ingests images from multiple Raspberry Pis (via MQTT), analyzes human posture with MediaPipe + OpenCV, stores annotated results, and **autoâ€‘schedules Kubernetes Jobs** to the **leastâ€‘busy GPU nodes** using **KEDA** with a **custom gRPC External Scaler** that reads **Prometheus** metrics.

> **Whatâ€™s new (updated scheduling):**
>
> - **Initial placement = even spread** across all available GPU nodes. If the number of ScaledJobs doesnâ€™t divide evenly, the **extra** job(s) go to the node(s) with the **lowest GPU usage**, tieâ€‘broken **randomly**.
> - A new **gpu_balancer.py** performs a **30â€‘second observation window**, computes **perâ€‘node average GPU usage**, and **offloads at most one job per overloaded node per loop** to the lowestâ€‘usage **underâ€‘threshold** node (random tie on equals). This repeats until all nodes are **stable** (â‰¤ threshold).
> - The External Scaler (`cpu_monitor_and_offload.py`) is **debounced** so KEDA polling doesnâ€™t thrash jobs while the balancer is stabilizing.

---

## âœ¨ Features

- **Distributed posture detection** with MediaPipe Pose + OpenCV.
- **Perâ€‘Pi ScaledJobs** (one KEDA ScaledJob per Pi/topic) for isolation and control.
- **GPUâ€‘aware scheduling** driven by Prometheus GPU usage metrics.
- **Even initial spread** + **gentle rebalancing** based on 30s averages.
- **Batteriesâ€‘included** scripts to build/push images, patch/apply manifests, run/stop safely.

---

## ğŸ—ºï¸ Architecture

```
Raspberry Pi(s)
   â”‚ (MQTT)
   â–¼
Analyzer Pods (Images_From_Pi*.py)  â†’  Annotated frames + posture metrics  â†’  Database (e.g., Supabase)

Prometheus  â† node exporters on Jetsons (GPU usage % per node)

KEDA (ScaledJobs)  â‡„  gRPC  â‡„  External Scaler (cpu_monitor_and_offload.py)
                               â”‚
                               â””â”€ launches â†’  gpu_balancer.py  (observe 30s â†’ offload 1 per overloaded node per loop)

Initial placement helper: patch_scaledjob.py  â†’ injects nodeAffinity for **even spread** (extras â†’ lowest usage; tie random)
```

---

## âš–ï¸ Scheduling Lifecycle (updated logic)

### 1) **Initial placement (firstâ€‘time scheduling)**

When you apply the ScaledJobs:
1. `patch_scaledjob.py` queries Prometheus to get a **current snapshot** of GPU usage per node.
2. It plans an **even distribution** of ScaledJobs across all available nodes:
   - Each node gets either âŒŠN/MâŒ‹ or âŒˆN/MâŒ‰ jobs, where N = number of ScaledJobs and M = number of nodes with valid metrics.
   - Any **extra** jobs are assigned to the node(s) with the **lowest** measured usage; if multiple nodes tie at that minimum, the script **chooses randomly** among those nodes to prevent dogâ€‘piling.
3. It writes `patched-job-*.yaml` with an injected:
   ```yaml
   affinity:
     nodeAffinity:
       requiredDuringSchedulingIgnoredDuringExecution:
         nodeSelectorTerms:
         - matchExpressions:
           - key: gpu-node
             operator: In
             values: [<chosen-node>]
   ```
4. Applying `patched-job-*.yaml` ensures the **first** Job per ScaledJob lands on its assigned node.

### 2) **Runtime balancing (observe â†’ gently offload â†’ repeat)**

Once KEDA reports **Active=True** and pods are running, `gpu_balancer.py` enters a loop:

1. **Observe 30s window**: sample GPU usage every few seconds (default 5s) and compute a **perâ€‘node average** over **30 seconds**.
2. **Check threshold** (default **50%**, configurable):
   - If **all nodes â‰¤ threshold** â†’ **stable**, do nothing, sleep 30s, and observe again.
   - If **one or more nodes > threshold** â†’ each such node is **overloaded**.
3. **Offload** (one per overloaded node per loop):
   - For **each overloaded node**, select **one running posture job/pod** on that node.
   - **Pick a destination node** from those **â‰¤ threshold** with the **lowest average usage** (random tieâ€‘break).
   - **Delete the Job(s)** for that ScaledJob (controller cleans up), **delete leftover pods** if any, **patch that ScaledJobâ€™s `nodeAffinity`** to the destination node.
4. **Repeat**: sleep 30s, recompute perâ€‘node averages, and repeat until **no node exceeds the threshold**.

This **â€œoneâ€‘offloadâ€‘perâ€‘overloadedâ€‘node per loopâ€** rule avoids thrashing and slowly converges the system to a balanced state.

### 3) **KEDA integration (debounced)**

- **KEDA â†” External Scaler**:  
  - `IsActive` returns **True** if **any** node has usage **below** the threshold; otherwise **False**.  
  - `GetMetricSpec` advertises `gpu_trigger` with `targetSize: 1`.  
  - `GetMetrics` returns a synthetic capacity (`sum(max(0, 100 - usage))`).

- **Debounce during stabilization**:  
  The scaler keeps returning **Active=True** for a short **stabilize window** after it detects capacity, so KEDA doesnâ€™t tear down jobs while `gpu_balancer.py` is still rebalancing. This prevents the â€œdelete every 30sâ€ flapping you may see in naÃ¯ve setups.

---

## ğŸ§© Repository Layout (updated)

### Orchestration & Control

- **`build_and_push.py`**  
  - Builds & pushes **multiâ€‘arch** analyzer images (`--platform linux/amd64,linux/arm64`).  
  - Taints the master (e.g., `nuc`) to repel posture jobs.  
  - Runs **`patch_scaledjob.py`** (even initial spread) â†’ writes `patched-job-*.yaml`.  
  - Applies all `patched-job-*.yaml`.  
  - Starts the **External Scaler** (gRPC on `:50051`) and an ENTERâ€‘toâ€‘stop helper.

- **`stop.py`**  
  - On ENTER: `docker-compose down`, kills scaler/balancer helpers, **deletes Jobs first** then leftover Pods, optionally deletes ScaledJobs, and **untaints** the master. (Updated to use robust matching and prefixes.)

### Scheduling & Scaling Brains

- **`cpu_monitor_and_offload.py`** â€” **External Scaler (gRPC server for KEDA)**  
  - `IsActive` (debounced), `GetMetricSpec`, `GetMetrics`.  
  - Reads GPU usage via Prometheus.  
  - Ensures `gpu_balancer.py` is running on Active.  
  - On sustained global overload may return inactive (no frantic deletions during stabilization).

- **`gpu_balancer.py`** â€” **30â€‘second average balancer** *(new)*  
  - Observes **30s**, computes perâ€‘node averages, compares against the threshold.  
  - For each **overloaded** node, **offloads at most one** job/pod to the **lowestâ€‘usage underâ€‘threshold** node (random tieâ€‘break), by patching that ScaledJobâ€™s `nodeAffinity` and recreating its Job.  
  - Repeats until the cluster is **stable**.

- **`patch_scaledjob.py`** â€” **Even initial spread** *(updated)*  
  - Reads **instantaneous** GPU usage per node.  
  - Distributes all ScaledJobs **evenly**; places any remainder on the **lowestâ€‘usage** nodes; random tieâ€‘break.  
  - Injects `nodeAffinity` into base YAMLs â†’ `patched-job-*.yaml`.

### Kubernetes Manifests (KEDA)

- **`posture-job-*.yaml`** â€” Base ScaledJob templates (preâ€‘patch).  
  - `triggers[0].type: external` with `metadata.scalerAddress: <HOST|IP>:50051`.  
  - One ScaledJob per Pi stream (`pi1`, `pi1-1`, `pi2-3`, `pi3`, â€¦).  
  - Container runs the matching `Images_From_Pi*.py`.  
  - DB env injected via env vars (migrate to Secrets for production).

- **`patched-job-*.yaml`** â€” Rendered ScaledJobs with **nodeAffinity** set per the evenâ€‘spread plan.

### Analyzer Apps

- **`Images_From_Pi*.py`** â€” perâ€‘stream analyzers:  
  - Subscribe to MQTT topic, decode image, run MediaPipe Pose.  
  - Compute **neck/body angles**, label posture **Good/Bad**, overlay graphics.  
  - Save annotated frames and write telemetry/metrics to the DB.

### gRPC / Misc

- **`externalscaler.proto`**, **`externalscaler_pb2.py`**, **`externalscaler_pb2_grpc.py`** â€” KEDA External Scaler API & generated stubs.  
- **`docker-compose.yml`** â€” optional local run (no K8s), mounting `./analyzed_images`.  
- **`Dockerfile`** â€” uses `--build-arg APP=<script.py>` to bake each analyzer variant.  
- **`last_node.txt`** â€” small state file used by the balancer/watcher to remember the last â€œbestâ€ node and avoid flapping.

---

## âš™ï¸ Configuration

### Prometheus Metrics Map
All 3 scripts use a `GPU_QUERIES` map. Update IPs/metric names to your exporters:
```python
GPU_QUERIES = {
  "agx-desktop": {"instance": "192.168.1.135:9100", "metric": "jetson_gpu_usage_percent"},
  "orin-desktop": {"instance": "192.168.1.118:9100", "metric": "jetson_orin_gpu_load_percent"},
  # ...
}
```

### Balancer / Scaler Knobs
- **`GPU_THRESHOLD`** (default `50`): node considered **overloaded** if avg usage > threshold.  
- **Window**: `gpu_balancer.py` uses **30s** (sample every 5s by default).  
- **Max offload rate**: **1 job per overloaded node per loop**.  
- **Debounce** (scaler): small stabilization window so KEDA keeps `Active=True` while balancing.

> Tip: You can make `GPU_THRESHOLD` an environment variable consumed by both `gpu_balancer.py` and `cpu_monitor_and_offload.py` for clusterâ€‘wide tuning.

### Node Labels
Nodes must be labeled with `gpu-node=<nodeName>`. The manifests/patcher depend on this for `nodeAffinity`.

### External Scaler Address
Each ScaledJob has:
```yaml
triggers:
- type: external
  metadata:
    scalerAddress: <HOST_OR_IP>:50051
```
Ensure the scaler is reachable from the cluster (you can run it on a node or expose it via a Service).

---

## â–¶ï¸ Run (Kubernetes)

1) **Build, patch, apply, start scaler**
   ```bash
   python3 build_and_push.py
   ```
   - Builds/pushes images for all analyzer variants.
   - Patches base YAMLs for **even initial spread**.
   - Applies `patched-job-*.yaml`.
   - Starts the External Scaler (gRPC on `:50051`) + ENTERâ€‘toâ€‘stop helper.

2) **Check status**
   ```bash
   kubectl get scaledjobs
   kubectl get jobs
   kubectl get pods -o wide
   ```

3) **Stop**
   Press **ENTER** in the `build_and_push.py` terminal. The helper will kill processes and **delete Jobs first** (then Pods), and untaint the master node.

---

## â–¶ï¸ Run Locally (Compose)

```bash
docker-compose up -d
# test image processing pipeline & DB writes
docker-compose down
```
Outputs are stored in `./analyzed_images`.

---

## ğŸ§ª Posture Logic (Analyzer)

- MediaPipe Pose â†’ key landmarks.  
- **Angles**: neck (earâ€“shoulder vs vertical), body (hipâ€“shoulder vs vertical).  
- **Label**: â€œGoodâ€/â€œBadâ€ via thresholds.  
- **Artifacts**: overlayed frames + DB rows (angles, status, timestamps, Pi ID, host).

---

## ğŸ” Security

- Move DB credentials from plainâ€‘text YAML/compose into **Kubernetes Secrets** or `.env` files not committed to VCS.  
- Restrict access to the scaler port (`:50051`) to cluster nodes only (e.g., ClusterIP Service + NetworkPolicy).

---

## ğŸ› ï¸ Troubleshooting

- **Pods churn at `pollingInterval` cadence** â†’ ensure the External Scalerâ€™s debounce is active and that `gpu_balancer.py` is running; check Prometheus metric availability.  
- **No movements** â†’ ensure node labels match, Prometheus is reachable, and the service account has rights to `kubectl patch` and `kubectl delete job/pod`.  
- **All nodes overloaded** â†’ balancer wonâ€™t offload; add capacity or reduce number of ScaledJobs.

---

## ğŸ“„ License

MIT (or your preferred license).

---

## ğŸ™Œ Acknowledgements

Built with [KEDA](https://keda.sh/) External Scaler gRPC pattern, Prometheus, and MediaPipe.
